import requests
from urllib.parse import urlparse, urljoin
import re
from bs4 import BeautifulSoup
from requests.exceptions import RequestException

# List of known malicious domains (expand as needed)
MALICIOUS_DOMAINS = ["malicious.com", "phishing.com", "evilsite.net"]
SUSPICIOUS_JS_PATTERNS = [r"eval\(", r"base64_decode\(", r"document\.location", r"window\.location"]
SUSPICIOUS_FORM_ACTIONS = [r"submit.*(php|cgi|exe)", r"action\s*=\s*['\"].*login.*['\"]"]

def check_malicious_domain(url):
    """
    Check if the URL belongs to any known malicious domains.
    """
    parsed_url = urlparse(url)
    domain = parsed_url.netloc
    for malicious_domain in MALICIOUS_DOMAINS:
        if malicious_domain in domain:
            return True
    return False

def check_suspicious_js(content):
    """
    Check if the webpage contains suspicious JavaScript patterns.
    """
    for pattern in SUSPICIOUS_JS_PATTERNS:
        if re.search(pattern, content):
            return True
    return False

def check_suspicious_forms(soup):
    """
    Check if the webpage contains suspicious forms.
    """
    forms = soup.find_all('form')
    for form in forms:
        action = form.get('action', '')
        if any(re.search(pattern, action, re.IGNORECASE) for pattern in SUSPICIOUS_FORM_ACTIONS):
            return True
    return False

def check_external_scripts(soup, base_url):
    """
    Check if the webpage contains scripts loaded from suspicious external domains.
    """
    scripts = soup.find_all('script', src=True)
    for script in scripts:
        script_url = urljoin(base_url, script['src'])
        parsed_url = urlparse(script_url)
        domain = parsed_url.netloc
        if any(malicious_domain in domain for malicious_domain in MALICIOUS_DOMAINS):
            return True
    return False

def check_for_malicious_links(soup, base_url):
    """
    Check if the webpage contains links to malicious sites.
    """
    links = soup.find_all('a', href=True)
    for link in links:
        link_url = urljoin(base_url, link['href'])
        parsed_url = urlparse(link_url)
        domain = parsed_url.netloc
        if any(malicious_domain in domain for malicious_domain in MALICIOUS_DOMAINS):
            return True
    return False

def scan_website(url):
    """
    Scan the website for potential malware indicators.
    """
    malware_found = False
    try:
        print(f"Scanning {url} for malware indicators...\n")
        response = requests.get(url)

        # Check if the request was successful
        if response.status_code == 200:
            print("Website loaded successfully.")
            content = response.text
            base_url = urlparse(url)._replace(path='/').geturl()

            # Parse the content with BeautifulSoup for easier manipulation
            soup = BeautifulSoup(content, 'html.parser')

            # Check for malicious domain
            if check_malicious_domain(url):
                print("Malware found: Malicious domain detected.")
                malware_found = True

            # Check for suspicious JavaScript patterns
            if check_suspicious_js(content):
                print("Malware found: Suspicious JavaScript detected.")
                malware_found = True

            # Check for suspicious forms
            if check_suspicious_forms(soup):
                print("Malware found: Suspicious form detected.")
                malware_found = True

            # Check for external scripts from suspicious sources
            if check_external_scripts(soup, base_url):
                print("Malware found: Suspicious external script detected.")
                malware_found = True

            # Check for malicious outbound links
            if check_for_malicious_links(soup, base_url):
                print("Malware found: Malicious outbound link detected.")
                malware_found = True

            if not malware_found:
                print("No malware detected. The website appears clean.")

        else:
            print(f"Error: Unable to load the website. Status code: {response.status_code}")

    except RequestException as e:
        print(f"Error: Could not scan the website. {e}")

if __name__ == "__main__":
    # Enter the website URL to scan
    url = input("Enter the website URL to scan for malware: ")
    scan_website(url)
